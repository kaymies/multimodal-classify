# -*- coding: utf-8 -*-
"""preprocess_spectrogram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PCf8iTTQNoosJl29TaODuTPKV4dtpTTt
"""

# creates a pickle file of spectrograms with associated labels

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
!pip install torch>=1.2.0
!pip install torchaudio
# %matplotlib inline
!pip install moviepy

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import Dataset
import torchaudio
import pandas as pd
import os
from google.colab import files
import pickle
import matplotlib.pyplot as plt
import moviepy.editor as mp 
import math

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # see whether gpu can be used
print(device)

aud_list = [] # initialize a list for waveforms to be added
# iterate through the folder structure to convert .mp4 files to .wav, then convert to waveform
for obj in range(12):
    for mat in range(5):
        for root, dirs, files in os.walk("/content/gdrive/My Drive/fall/{}/{}/".format(obj,mat), topdown=False): # change directory as necessary
            for name in files:
                full_file_name = os.path.join(root, name) # file name of each file in this folder including the root
                if full_file_name.endswith("Camera_1.mp4"): # we only care about the Camera_1.mp4 file
                    clip = mp.VideoFileClip(r"{}".format(full_file_name))
                    clip.audio.write_audiofile(r"{}/Camera_1.wav".format(root)) # convert .mp4 to .wav
                    waveform, sample_rate = torchaudio.load("{}/Camera_1.wav".format(root)) # convert .wav to waveform
                    aud_list.append(waveform[0])

# convert waveform to spectrogram
spec_list = []
for i in range(len(aud_list)):
  spec_list.append(torchaudio.transforms.Spectrogram()(aud_list[i]))

# reshape spectrogram to what the reflection pad function expects
spec_list_unsqueeze = []
for i in range(len(spec_list)):
    x = torch.unsqueeze(spec_list[i], 0)
    x = torch.unsqueeze(x,0)
    spec_list_unsqueeze.append(x)

# create spectrogram that is padded using reflection
spec_list_padded = []
for i in range(len(spec_list_unsqueeze)):
    diff = 1600 - spec_list[i].size()[1] # 1600 is target shape of spectrogram in horizontal direction
    leftover = diff
    padded = spec_list_unsqueeze[i]
    while math.ceil(leftover/2) > spec_list[i].size()[1]: # stay in this loop until the amount of padding is less than the original spectrogram size
        temp_pad = nn.ReflectionPad2d((spec_list[i].size()[1]-1, spec_list[i].size()[1]-1, 0, 0)) # create padding that is the size of original spectrogram - 1
        padded = temp_pad(padded)
        leftover -= 2*spec_list[i].size()[1]-2
    m = nn.ReflectionPad2d((math.floor(leftover/2), math.ceil(leftover/2), 27, 28)) # create final padding
    padded_final = m(padded) # apply final padding
    spec_list_padded.append(padded_final)

# visualize one spectrogram
plt.figure()
plt.imshow(spec_list_padded[1].log2()[:,:].numpy(), cmap='gray')

phys101_df = pd.read_csv('/content/gdrive/My Drive/Phys101.csv', sep=',', header=None) # read off the labels from csv
phys101_df.fillna(0) # turn any NaN into 0
phys101spec_df['Sound'] = spec_list_padded # add a 'Sound' column with spectrograms to dataframe
phys101spec_df.to_pickle("/content/gdrive/My Drive/phys101spec.pkl") # save to pickle for future use