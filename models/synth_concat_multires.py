# -*- coding: utf-8 -*-
"""synth_concat_multires.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12xXKyUs5pRLwyZUmFsVcs-NKMUsQbAIC
"""

# combines soundnet and resnet with multiple video frames using simple concatenation

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
!pip install torch>=1.2.0
!pip install torchaudio
# %matplotlib inline

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import Dataset
import torchaudio
import pandas as pd
import os
from google.colab import files
import pickle
import matplotlib.pyplot as plt
import math
from PIL import Image
from google.colab.patches import cv2_imshow
import sys
import argparse
import cv2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # see whether gpu can be used
print(device)

# load appropriate pickle with waveform, multiple frames from video, and labels
phys101_merged_df = pd.read_pickle("/content/gdrive/My Drive/phys101merged.pkl")
phys101_df_shuffled = phys101_merged_df.sample(frac=1) # shuffle the data

# create dataset class
class AVDataset(Dataset):
    def __init__(self, df):
        self.labels = np.asarray(df[[0, 1]]) # make labels (obj, mat)
        self.df = df
        self.waveforms = np.asarray(df['Sound']) # input

    def __getitem__(self, index):
        # return self.waveforms[index], self.labels[index, :]
        inputs = []
        for i in range(4):
          input_image = Image.open(np.asarray(self.df['frame{}'.format(i)])[index])
          preprocess = transforms.Compose([
              transforms.Resize(256),
              transforms.CenterCrop(224),
              transforms.ToTensor(),
              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
          ])
          input_tensor = preprocess(input_image)
          inputs.append(input_tensor.unsqueeze(0)) # create a mini-batch as expected by the model
        return self.waveforms[index], inputs[0], inputs[1], inputs[2], inputs[3], self.labels[index, :]

    def __len__(self):
        return len(self.waveforms)

train_set = AVDataset(phys101_df_shuffled[:533]) # ~80% of data for training
test_set = AVDataset(phys101_df_shuffled[533:]) # ~20% of data for testing

print("Train set size: " + str(len(train_set)))
print("Test set size: " + str(len(test_set)))

train_loader = torch.utils.data.DataLoader(train_set, batch_size = 10, shuffle = True) # make data type for training data
test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = True) # make data type for testing data

# this class is taken from https://github.com/keunhong/pytorch-soundnet
# CNNs are used to as sound is invariant to translations, and detect higher level
# concepts through lower level detectors
class SoundNet(nn.Module):
    def __init__(self):
        super(SoundNet, self).__init__()

        self.conv1 = nn.Conv2d(1, 16, kernel_size=(64, 1), stride=(2, 1), padding=(32, 0))
        self.batchnorm1 = nn.BatchNorm2d(16, eps=1e-5, momentum=0.1)
        self.relu1 = nn.ReLU(True)
        self.maxpool1 = nn.MaxPool2d((8, 1), stride=(8, 1))

        self.conv2 = nn.Conv2d(16, 32, kernel_size=(32, 1), stride=(2, 1), padding=(16, 0))
        self.batchnorm2 = nn.BatchNorm2d(32, eps=1e-5, momentum=0.1)
        self.relu2 = nn.ReLU(True)
        self.maxpool2 = nn.MaxPool2d((8, 1), stride=(8, 1))

        self.conv3 = nn.Conv2d(32, 64, kernel_size=(16, 1), stride=(2, 1), padding=(8, 0))
        self.batchnorm3 = nn.BatchNorm2d(64, eps=1e-5, momentum=0.1)
        self.relu3 = nn.ReLU(True)

        self.conv4 = nn.Conv2d(64, 128, kernel_size=(8, 1), stride=(2, 1), padding=(4, 0))
        self.batchnorm4 = nn.BatchNorm2d(128, eps=1e-5, momentum=0.1)
        self.relu4 = nn.ReLU(True)

        self.conv5 = nn.Conv2d(128, 256, kernel_size=(4, 1), stride=(2, 1), padding=(2, 0))
        self.batchnorm5 = nn.BatchNorm2d(256, eps=1e-5, momentum=0.1)
        self.relu5 = nn.ReLU(True)
        self.maxpool5 = nn.MaxPool2d((4, 1), stride=(4, 1))

        self.conv6 = nn.Conv2d(256, 512, kernel_size=(4, 1), stride=(2, 1), padding=(2, 0))
        self.batchnorm6 = nn.BatchNorm2d(512, eps=1e-5, momentum=0.1)
        self.relu6 = nn.ReLU(True)

        self.conv7 = nn.Conv2d(512, 1024, kernel_size=(4, 1), stride=(2, 1), padding=(2, 0))
        self.batchnorm7 = nn.BatchNorm2d(1024, eps=1e-5, momentum=0.1)
        self.relu7 = nn.ReLU(True)

        # this last layer is not being used, but is useful for variable length waveforms
        # as the output layer does not depend on the lenght of the signal
        self.conv8_objs = nn.Conv2d(1024, 1000, kernel_size=(8, 1), stride=(2, 1)) 
        self.conv8_scns = nn.Conv2d(1024, 401, kernel_size=(8, 1), stride=(2, 1))        

    # forward pass
    def forward(self, waveform):
        x = self.conv1(waveform)
        x = self.batchnorm1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)

        x = self.conv2(x)
        x = self.batchnorm2(x)
        x = self.relu2(x)
        x = self.maxpool2(x)

        x = self.conv3(x)
        x = self.batchnorm3(x)
        x = self.relu3(x)

        x = self.conv4(x)
        x = self.batchnorm4(x)
        x = self.relu4(x)

        x = self.conv5(x)
        x = self.batchnorm5(x)
        x = self.relu5(x)
        x = self.maxpool5(x)

        x = self.conv6(x)
        x = self.batchnorm6(x)
        x = self.relu6(x)

        x = self.conv7(x)
        x = self.batchnorm7(x)
        x = self.relu7(x)

        return x

# this is a wrapper model that we added to incorporate the audio and visual models to classify
# object and material types by flattening the output of the models, concatenating, and passing it through
# a fully connected layer
class OurSynth(nn.Module):
    def __init__(self, resnet, soundnet):
        super(OurSynth, self).__init__()
        self.features = nn.Sequential(
            *list(resnet.children())[:-1])
        self.flatten = nn.Flatten()
        self.fc_obj = nn.Linear(512*4+1024*11, 12) # size is conv7_outchannel x signal_length, output is the number of classes
        self.fc_mat = nn.Linear(512*4+1024*11, 5)
        self.soundnet = soundnet
        self.relu = nn.ReLU()
    def forward(self, x0, x1, x2, x3, x_sound):
        x0 = self.features(x0)
        x0 = self.flatten(x0) # flatten output
        x1 = self.features(x1)
        x1 = self.flatten(x1)
        x2 = self.features(x2)
        x2 = self.flatten(x2)
        x3 = self.features(x3)
        x3 = self.flatten(x3)
        x_sound = self.soundnet(x_sound) # output of SoundNet
        x_sound = self.flatten(x_sound)
        x = torch.cat((x0, x1, x2, x3, x_sound), dim=1) # concatenate
        x_obj = self.fc_obj(x) # fully connected layer for object label
        x_mat = self.fc_mat(x) # fully connected layer for material label
        return F.log_softmax(x_obj, dim=1), F.log_softmax(x_mat, dim=1) # apply log_softmax to find most likely class (log_softmax works best with NLL Loss)

# define the training mode
# takes in the model that's used, the epoch number, and the optimizer 
def train(model, epoch, optimizer):
    model.train()
    loss_list = [] # initialize loss list so we can average loss for each epoch
    for batch_idx, (data_sound, frame0, frame1, frame2, frame3, target) in enumerate(train_loader):
        data_sound = torch.unsqueeze(data_sound,1) # convert 3 dimensional data to 4 dimensional data that model expects
        data_sound = torch.unsqueeze(data_sound,3)
        frame0 = torch.squeeze(frame0)
        frame1 = torch.squeeze(frame1)
        frame2 = torch.squeeze(frame2)
        frame3 = torch.squeeze(frame3)
        optimizer.zero_grad()
        frame0 = frame0.to(device)
        frame1 = frame1.to(device)
        frame2 = frame2.to(device)
        frame3 = frame3.to(device)
        data_sound = data_sound.to(device)
        target = target.to(device).long() # pytorch wants target to be long() datatype
        output_obj, output_mat = model(frame0, frame1, frame2, frame3, data_sound) # separate the output into material, object, scene
        loss_obj = F.nll_loss(output_obj, target[:,0]) # the loss functions expects a batchSizexn_class input
        loss_mat = F.nll_loss(output_mat, target[:,1])
        loss = loss_obj + loss_mat # multiply (1:1, 2:1, 4:1)
        loss_list.append(loss)
        loss.backward() # back prop
        optimizer.step() # take a step with the chosen optimizer
        if batch_idx % log_interval == 0: #print training stats
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data_sound), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss))
    return sum(loss_list)/len(loss_list)

# define test mode
# takes in model used, epoch number, and data
def test(model, epoch, test_loader):
    model.eval() # switch to test/evaluaion mode
    correct_obj = 0
    correct_mat = 0 # initialize correct classification count   
    for data_sound, frame0, frame1, frame2, frame3, target in test_loader:
        data_sound = torch.unsqueeze(data_sound,1) # convert 3 dimensional data to 4 dimensional data that model expects
        data_sound = torch.unsqueeze(data_sound,3)
        frame0 = torch.squeeze(frame0)
        frame0 = torch.unsqueeze(frame0,0)
        frame0 = frame0.to(device)
        frame1 = torch.squeeze(frame1)
        frame1 = torch.unsqueeze(frame1,0)
        frame1 = frame1.to(device)
        frame2 = torch.squeeze(frame2)
        frame2 = torch.unsqueeze(frame2,0)
        frame2 = frame2.to(device)
        frame3 = torch.squeeze(frame3)
        frame3 = torch.unsqueeze(frame3,0)
        frame3 = frame3.to(device)
        data_sound = data_sound.to(device)
        target = target.to(device).long() # pytorch wants target to be long() datatype
        output_obj, output_mat = model(frame0, frame1, frame2, frame3, data_sound)
        pred_obj = torch.argmax(output_obj, dim=1)
        correct_obj += torch.sum(pred_obj == target[:,0])
        pred_mat = torch.argmax(output_mat, dim=1) # get the index of the max log-probability
        correct_mat += torch.sum(pred_mat == target[:,1]) # count as correct if the above index matches the target label
        print('\nTest set: Object Accuracy: {}/{} ({:.0f}%)\n Material Accuracy: {}/{} ({:.0f}%)'.format(
          correct_obj, len(test_loader.dataset),
          100. * correct_obj / len(test_loader.dataset),
          correct_mat, len(test_loader.dataset),
          100. * correct_mat / len(test_loader.dataset)))
    return correct_obj/len(test_loader.dataset), correct_mat/len(test_loader.dataset)

# load soundnet
soundnet_model = SoundNet()
soundnet_model.load_state_dict(torch.load('/content/gdrive/My Drive/soundnet8_final.pth')) # load the pretrained weights and biases
soundnet_model.to(device)

# load resnet
resnet_model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True) #increase resnet model layer
resnet_model.to(device)

# load combined model
synth_model = OurSynth(resnet_model, soundnet_model)
synth_model.to(device)

# initializing the optimizer
# to fully leverage the pretrained models, we are only updating the
# weights of the final fully connected layer in our wrapper model
params = list(synth_model.fc_obj.parameters()) + list(synth_model.fc_mat.parameters())
optimizer = optim.Adam(params, lr = 0.001)

log_interval = 1 # printing the loss every log_interval steps
train_list = [] # training loss for every epoch
test_list = [] # test accuracy for every epoch
for epoch in range(10):
    train_list.append(train(synth_model, epoch, optimizer)) # train
    test_list.append(test(synth_model, epoch, test_loader)) # test
torch.save(synth_model.state_dict(), '/content/gdrive/My Drive/phys101_synth_with_multires.pth') # save weights and biases

# plot training loss
plt.plot(train_list)
plt.xlabel('epoch')
plt.ylabel('training loss')

# plot validation accuracy
plt.plot(test_list)
plt.xlabel('epoch')
plt.ylabel('validation accuracy')