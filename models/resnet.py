# -*- coding: utf-8 -*-
"""resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v0YgaTsPzDZHFlxHV6ZE06zRBOVz8VTy
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount=False)

# Commented out IPython magic to ensure Python compatibility.
!pip install torch>=1.2.0
!pip install torchaudio
# %matplotlib inline

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms, io
from torch.utils.data import Dataset
import torchaudio
import pandas as pd
import os
from google.colab import files
import pickle
import matplotlib.pyplot as plt
import math
from PIL import Image
from google.colab.patches import cv2_imshow
import sys
import argparse
import cv2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # see whether gpu can be used
print(device)

# load appropriate pickle with images and corresponding labels
phys101_merged_df = pd.read_pickle("/content/gdrive/My Drive/phys101merged.pkl")

phys101_df_shuffled = phys101_merged_df.sample(frac=1) # shuffle the data

# create dataset class
class VisDataset(Dataset):
    def __init__(self, df):
        self.labels = np.asarray(df[[0, 1]]) # make labels (obj, mat)
        self.df = df
        self.waveforms = np.asarray(df['Sound']) # input

    def __getitem__(self, index):
        # return self.waveforms[index], self.labels[index, :]
        inputs = []
        for i in range(4):
          input_image = Image.open(np.asarray(self.df['frame{}'.format(i)])[index])
          preprocess = transforms.Compose([
              transforms.Resize(256),
              transforms.CenterCrop(224),
              transforms.ToTensor(),
              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
          ])
          input_tensor = preprocess(input_image)
          inputs.append(input_tensor.unsqueeze(0)) # create a mini-batch as expected by the model
        # return inputs[0], inputs[1], inputs[2], inputs[3], self.labels[index, :]
        return inputs[0], self.labels[index, :]

    def __len__(self):
        return len(self.waveforms)

train_set = VisDataset(phys101_df_shuffled[:533]) # ~80% of data for training
test_set = VisDataset(phys101_df_shuffled[533:]) # ~20% of data for testing

print("Train set size: " + str(len(train_set)))
print("Test set size: " + str(len(test_set)))

train_loader = torch.utils.data.DataLoader(train_set, batch_size = 40, shuffle = True) # make data type for training data
test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = True) # make data type for testing data

# this is a wrapper model that we added to incorporate ResNet-18 to classify
# material types by flattening the output of SoundNet and passing it through
# a fully connected layer
class OurVis(nn.Module):
    def __init__(self, resnet):
        super(OurVis, self).__init__()
        # self.resnet = resnet
        # self.flatten = nn.Flatten()
        self.features = nn.Sequential(
            *list(resnet.children())[:-1]) # all layers except for the last layer of ResNet
        self.flatten = nn.Flatten()
        self.fc_obj = nn.Linear(512, 12) # size is conv7_outchannel x signal_length, output is the number of classes
        self.fc_mat = nn.Linear(512, 5)
    def forward(self, x):
        x = self.features(x) # input is one image
        x = self.flatten(x) # flatten output
        x_obj = self.fc_obj(x) # fully connected layer for object label
        x_mat = self.fc_mat(x) # fully connected layer for material label
        return F.log_softmax(x_obj, dim=1), F.log_softmax(x_mat, dim=1) # apply log_softmax to find most likely class (log_softmax works best with NLL Loss)

# define the training mode
# takes in the model that's used, the epoch number, and the optimizer 
def train(model, epoch, optimizer):
    model.train()
    loss_list = [] # initialize loss list so we can average loss for each epoch
    # for batch_idx, (frame0, frame1, frame2, frame3, target) in enumerate(train_loader):
    for batch_idx, (data, target) in enumerate(train_loader):
        data = torch.squeeze(data) # reformat dimension to what the model expects
        optimizer.zero_grad()
        data = data.to(device)
        target = target.to(device).long() # pytorch wants target to be long() datatype
        output_obj, output_mat = model(data) # separate the output into material, object, scene
        loss_obj = F.nll_loss(output_obj, target[:,0]) # the loss functions expects a batchSizexn_class input
        loss_mat = F.nll_loss(output_mat, target[:,1])
        loss = loss_obj + loss_mat
        loss_list.append(loss)
        loss.backward() # back prop
        optimizer.step() # take a step with the chosen optimizer
        if batch_idx % log_interval == 0: #print training stats
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss))
    return sum(loss_list)/len(loss_list) # return average loss for this epoch for graphing purposes

# define test mode
# takes in model used, epoch number, and data
def test(model, epoch, test_loader):
    model.eval() # switch to test/evaluaion mode
    correct_obj = 0
    correct_mat = 0 # initialize correct classification count
    for data, target in test_loader:
        data = torch.squeeze(data) # reformat dimension to what the model expects
        data = torch.unsqueeze(data,0)
        data = data.to(device)
        target = target.to(device).long() # pytorch wants target to be long() datatype
        output_obj, output_mat = model(data)
        pred_obj = torch.argmax(output_obj, dim=1)
        correct_obj += torch.sum(pred_obj == target[:,0])
        pred_mat = torch.argmax(output_mat, dim=1) # get the index of the max log-probability
        correct_mat += torch.sum(pred_mat == target[:,1]) # count as correct if the above index matches the target label
        print('\nTest set: Object Accuracy: {}/{} ({:.0f}%)\n Material Accuracy: {}/{} ({:.0f}%)'.format(
          correct_obj, len(test_loader.dataset),
          100. * correct_obj / len(test_loader.dataset),
          correct_mat, len(test_loader.dataset),
          100. * correct_mat / len(test_loader.dataset)))
    return correct_obj/len(test_loader.dataset), correct_mat/len(test_loader.dataset)

# load pretrained resnet model
resnet_model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)
resnet_model.eval()
ourVis_model = OurVis(resnet_model)
ourVis_model.to(device)

# initializing the optimizer
# to fully leverage the pretrained ResNet model, we are only updating the
# final layer of ResNet, as well as
# weights of the final fully connected layer in our wrapper model
params = list(ourVis_model.features[-1].parameters()) + list(ourVis_model.fc_obj.parameters()) + list(ourVis_model.fc_mat.parameters())
optimizer = optim.Adam(params, lr = 0.001)

log_interval = 1 # printing the loss every log_interval steps
train_loss = [] # training loss for every epoch
accuracy = [] # test accuracy for every epoch
for epoch in range(2):
    train_loss.append(train(ourVis_model, epoch, optimizer)) # train
    accuracy.append(test(ourVis_model, epoch, test_loader)) # test
torch.save(ourVis_model.state_dict(), '/content/gdrive/My Drive/phys101_resnet.pth') # save model weights and biases

# plot training loss
plt.plot(train_loss)
plt.xlabel('epoch')
plt.ylabel('training loss')

# plot validation accuracy
plt.plot(accuracy)
plt.xlabel('epoch')
plt.ylabel('validation accuracy')